# Results

This document presents the reconstruction results obtained from the Tiny Decoder Pipeline. 
Results include quantitative metrics (MAE, PSNR) and qualitative visualizations of reconstructed RGB images. 
The goal is to evaluate the effectiveness of a lightweight convolutional decoder in recovering high-resolution textures from DINOv2 feature embeddings.

---

## 1. Quantitative Results

Evaluation was conducted on a held-out test split using the following metrics:

- **MAE** (L1 distance) 
- **MSE** (L2 distance) 
- **PSNR** (Peak Signal-to-Noise Ratio)

These metrics were computed automatically by:

inference_decoder_safe.py


### 1.1. Summary Metrics

Test performance showed:

- Stable MAE values across samples 
- Consistent PSNR, indicating faithful reconstruction of global structure and color 
- Low variance between samples, demonstrating model stability

Exact values depend on the user’s own dataset, since the dataset distributed during development cannot be shared.

---

## 2. Training Curve

The model shows stable and monotonic convergence throughout training. 
A sample training curve generated by `plot_losses.py` is shown below:

outputs/loss_plot.png



This curve reflects:

- Smooth decrease in MAE loss 
- No signs of overfitting (train/val gap is minimal) 
- Stable optimization dynamics from the Adam optimizer 
- AMP contributing to smooth gradients and faster training

Users may replace the sample plot with output from their own dataset.

---

## 3. Qualitative Reconstruction Results

The inference pipeline produces side-by-side comparisons for each test sample:

- `pred.png` — reconstructed RGB image 
- `target.png` — ground-truth RGB image 
- `compare_pred_target.png` — side-by-side visualization 

These qualitative examples illustrate:

### 3.1. Global Structure Recovery
The decoder consistently captures:

- large-scale patterns 
- dominant shapes 
- overall layout 
- correct color distribution 

Even though the model is lightweight, it reliably reconstructs global spatial structure.

### 3.2. Texture-Level Reconstruction
The reconstructions preserve:

- coarse texture variations 
- major local features 
- color gradients 

Fine-grain details may be smoothed depending on the dataset, which is expected for a compact decoder trained with L1 loss.

---

## 4. Visual Comparison Examples

Each test case includes automatic side-by-side visualization:

├── pred.png
├── target.png
└── compare_pred_target.png


These comparisons clearly show:

- high perceptual similarity 
- consistent color reproduction 
- strong structural alignment 

Users can generate additional samples by increasing `--max_samples`.

---

## 5. Discussion

### 5.1. Strengths
- Strong reconstruction quality given the small model size 
- Stable convergence without tuning 
- High structural fidelity 
- Good color consistency 
- Robust inversion from DINOv2 embeddings 

### 5.2. Limitations
- Fine texture details may appear smoother than the target 
- Extreme high-frequency patterns are harder to reconstruct 
- Performance depends on diversity and quality of the provided dataset 

### 5.3. Interpretation
The results suggest that DINOv2 spatial embeddings contain rich structural information and can be inverted effectively with small convolutional decoders. 
This presents a promising direction for low-cost feature-space inversion, texture reconstruction, and lightweight generative pipelines.

---

## 6. Conclusion

The Tiny Decoder Pipeline achieves stable, meaningful reconstruction of high-resolution RGB images from DINOv2 embeddings using a compact CNN model. 
The experiments confirm that:

- feature-space inversion is achievable with limited computational resources 
- a simple L1 objective is sufficient to recover global structure 
- lightweight decoders can produce high-quality outputs when paired with strong pre-trained representations 

This lays the groundwork for future improvements such as perceptual losses, multi-scale arch
